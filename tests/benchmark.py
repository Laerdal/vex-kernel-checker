#!/usr/bin/env python3
"""
Performance benchmarking script for VEX Kernel Checker.

This script runs performance tests using the main CLI script to measure
execution times for real-world operations.
"""

import sys
import os
import time
import json
import argparse
import statistics
import subprocess
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))


class BenchmarkRunner:
    """Main benchmark runner class."""
    
    def __init__(self, quiet=False, iterations=3):
        self.quiet = quiet
        self.iterations = iterations
        self.results = {}
        self.test_data_dir = project_root / "examples"
        self.main_script = project_root / "vex-kernel-checker.py"
        
    def log(self, message):
        """Log message if not in quiet mode."""
        if not self.quiet:
            print(message)
    
    def run_timed_operation(self, name, operation, *args, **kwargs):
        """Run an operation multiple times and collect timing statistics."""
        self.log(f"\nüî¨ Benchmarking: {name}")
        times = []
        
        for i in range(self.iterations):
            self.log(f"  Iteration {i+1}/{self.iterations}")
            start_time = time.time()
            try:
                result = operation(*args, **kwargs)
                end_time = time.time()
                duration = end_time - start_time
                times.append(duration)
                self.log(f"    Duration: {duration:.3f}s")
            except Exception as e:
                self.log(f"    ‚ùå Error: {e}")
                return None
        
        if times:
            stats = {
                'operation': name,
                'iterations': len(times),
                'mean': statistics.mean(times),
                'median': statistics.median(times),
                'min': min(times),
                'max': max(times),
                'stdev': statistics.stdev(times) if len(times) > 1 else 0.0,
                'times': times
            }
            
            self.results[name] = stats
            self.log(f"  üìä Results: mean={stats['mean']:.3f}s, "
                    f"median={stats['median']:.3f}s, "
                    f"min={stats['min']:.3f}s, max={stats['max']:.3f}s")
            return stats
        
        return None
    
    def run_cli_command(self, args):
        """Run the main CLI script with given arguments."""
        cmd = [sys.executable, str(self.main_script)] + args
        result = subprocess.run(cmd, capture_output=True, text=True, cwd=project_root)
        return result.returncode == 0
    
    def benchmark_vex_analysis(self):
        """Benchmark VEX file analysis using CLI."""
        self.log("\nüîç Benchmarking VEX Analysis")
        
        # Test with different VEX file sizes
        test_files = [
            "test_small_vex.json",
            "test_real_cve.json", 
            "test_mixed_vex.json"
        ]
        
        kernel_source = self.test_data_dir / "test_kernel_source"
        if not kernel_source.exists():
            self.log("  ‚ö†Ô∏è  Skipping VEX analysis (test kernel source not found)")
            return
        
        for test_file in test_files:
            vex_path = self.test_data_dir / test_file
            if not vex_path.exists():
                self.log(f"  ‚ö†Ô∏è  Skipping {test_file} (not found)")
                continue
            
            # Create a test config file if it doesn't exist
            config_files = [
                kernel_source / ".config",
                self.test_data_dir / "test_demo.config",
                kernel_source / "sample.config"
            ]
            
            config_path = None
            for config_file in config_files:
                if config_file.exists():
                    config_path = config_file
                    break
            
            if not config_path:
                self.log(f"  ‚ö†Ô∏è  Skipping {test_file} (no config file)")
                continue
            
            # Benchmark full analysis
            self.run_timed_operation(
                f"Full VEX Analysis ({test_file})",
                self.run_cli_command,
                [
                    "--vex-file", str(vex_path),
                    "--kernel-config", str(config_path),
                    "--kernel-source", str(kernel_source),
                    "--config-only",
                    "--quiet"
                ]
            )
    
    def benchmark_config_analysis(self):
        """Benchmark configuration analysis using CLI."""
        self.log("\n‚öôÔ∏è  Benchmarking Config Analysis")
        
        kernel_source = self.test_data_dir / "test_kernel_source"
        if not kernel_source.exists():
            self.log("  ‚ö†Ô∏è  Skipping config analysis (test kernel source not found)")
            return
        
        # Find a valid config file
        config_files = [
            kernel_source / ".config",
            self.test_data_dir / "test_demo.config",
            kernel_source / "sample.config",
            self.test_data_dir / "test_kernel_source" / ".config"
        ]
        
        config_path = None
        for config_file in config_files:
            if config_file.exists():
                config_path = config_file
                break
        
        if not config_path:
            self.log("  ‚ö†Ô∏è  Skipping config analysis (no config file found)")
            return
        
        vex_path = self.test_data_dir / "test_small_vex.json"
        if not vex_path.exists():
            self.log("  ‚ö†Ô∏è  Skipping config analysis (no VEX file found)")
            return
        
        # Benchmark config-only mode
        self.run_timed_operation(
            "Config-Only Analysis",
            self.run_cli_command,
            [
                "--vex-file", str(vex_path),
                "--kernel-config", str(config_path),
                "--kernel-source", str(kernel_source),
                "--config-only",
                "--quiet"
            ]
        )
    
    def benchmark_filesystem_analysis(self):
        """Benchmark filesystem analysis using CLI.""" 
        self.log("\nÔøΩÔ∏è  Benchmarking Filesystem Analysis")
        
        kernel_source = self.test_data_dir / "test_kernel_source"
        if not kernel_source.exists():
            self.log("  ‚ö†Ô∏è  Skipping filesystem analysis (test kernel source not found)")
            return
        
        vex_path = self.test_data_dir / "test_small_vex.json"
        if not vex_path.exists():
            self.log("  ‚ö†Ô∏è  Skipping filesystem analysis (no VEX file found)")
            return
        
        config_path = kernel_source / ".config"
        if not config_path.exists():
            config_path = self.test_data_dir / "test_demo.config"
            if not config_path.exists():
                config_path = kernel_source / "sample.config"
                if not config_path.exists():
                    self.log("  ‚ö†Ô∏è  Skipping filesystem analysis (no config file)")
                    return
        
        # Benchmark filesystem mode (without config-only)
        self.run_timed_operation(
            "Filesystem Analysis",
            self.run_cli_command,
            [
                "--vex-file", str(vex_path),
                "--kernel-config", str(config_path),
                "--kernel-source", str(kernel_source),
                "--quiet"
            ]
        )
    
    def benchmark_help_and_validation(self):
        """Benchmark help and basic validation operations."""
        self.log("\n‚ùì Benchmarking Help and Validation")
        
        # Benchmark help display
        self.run_timed_operation(
            "Help Display",
            self.run_cli_command,
            ["--help"]
        )
        
        # Benchmark version display
        self.run_timed_operation(
            "Version Display", 
            self.run_cli_command,
            ["--version"]
        )
    
    def run_full_benchmark(self):
        """Run all benchmark tests."""
        self.log("üöÄ Starting VEX Kernel Checker Performance Benchmarks")
        self.log(f"   Iterations per test: {self.iterations}")
        self.log(f"   Quiet mode: {self.quiet}")
        
        start_time = time.time()
        
        # Run all benchmark categories
        self.benchmark_vex_analysis()
        self.benchmark_config_analysis() 
        self.benchmark_filesystem_analysis()
        self.benchmark_help_and_validation()
        
        end_time = time.time()
        total_duration = end_time - start_time
        
        self.log(f"\n‚úÖ Benchmarks completed in {total_duration:.2f} seconds")
        
        return self.results
    
    def save_results(self, output_file="benchmark_results.json"):
        """Save benchmark results to JSON file."""
        output_path = project_root / output_file
        
        # Add metadata
        results_with_metadata = {
            'benchmark_metadata': {
                'timestamp': time.time(),
                'iterations': self.iterations,
                'python_version': sys.version,
                'platform': sys.platform
            },
            'results': self.results
        }
        
        try:
            with open(output_path, 'w') as f:
                json.dump(results_with_metadata, f, indent=2)
            self.log(f"\nüíæ Results saved to {output_path}")
        except Exception as e:
            self.log(f"\n‚ùå Error saving results: {e}")
    
    def print_summary(self):
        """Print a summary of benchmark results."""
        if not self.results:
            self.log("No benchmark results to summarize.")
            return
        
        self.log("\nüìà Benchmark Summary")
        self.log("=" * 50)
        
        # Sort by mean time
        sorted_results = sorted(
            self.results.items(), 
            key=lambda x: x[1]['mean']
        )
        
        for name, stats in sorted_results:
            self.log(f"{name:30} | {stats['mean']:6.3f}s ¬± {stats['stdev']:6.3f}s")
        
        # Overall statistics
        all_times = [stats['mean'] for stats in self.results.values()]
        if all_times:
            self.log("\nüìä Overall Statistics")
            self.log(f"Total operations benchmarked: {len(all_times)}")
            self.log(f"Fastest operation: {min(all_times):.3f}s")
            self.log(f"Slowest operation: {max(all_times):.3f}s")
            self.log(f"Average operation time: {statistics.mean(all_times):.3f}s")


def main():
    """Main entry point for benchmark script."""
    parser = argparse.ArgumentParser(
        description="Run performance benchmarks for VEX Kernel Checker"
    )
    parser.add_argument(
        "--quiet", 
        action="store_true",
        help="Run in quiet mode with minimal output"
    )
    parser.add_argument(
        "--iterations",
        type=int,
        default=3,
        help="Number of iterations per benchmark (default: 3)"
    )
    parser.add_argument(
        "--output",
        default="benchmark_results.json",
        help="Output file for benchmark results (default: benchmark_results.json)"
    )
    parser.add_argument(
        "--no-save",
        action="store_true",
        help="Don't save results to file"
    )
    
    args = parser.parse_args()
    
    # Run benchmarks
    runner = BenchmarkRunner(quiet=args.quiet, iterations=args.iterations)
    results = runner.run_full_benchmark()
    
    # Print summary
    runner.print_summary()
    
    # Save results
    if not args.no_save:
        runner.save_results(args.output)
    
    # Return exit code based on whether we got results
    return 0 if results else 1


if __name__ == "__main__":
    sys.exit(main())
